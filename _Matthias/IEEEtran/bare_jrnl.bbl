% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{Krizhevsky2012}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton, ``Imagenet classification with
  deep convolutional neural networks,'' in \emph{Advances in neural information
  processing systems}, 2012, pp. 1097--1105.

\bibitem{Deng2009}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei, ``Imagenet: A
  large-scale hierarchical image database,'' in \emph{Computer Vision and
  Pattern Recognition, 2009. CVPR 2009. IEEE Conference on}.\hskip 1em plus
  0.5em minus 0.4em\relax IEEE, 2009, pp. 248--255.

\bibitem{Taigman2014}
Y.~Taigman, M.~Yang, M.~Ranzato, and L.~Wolf, ``Deepface: Closing the gap to
  human-level performance in face verification,'' in \emph{Proceedings of the
  IEEE conference on computer vision and pattern recognition}, 2014, pp.
  1701--1708.

\bibitem{Rajpurkar2017}
P.~Rajpurkar, A.~Y. Hannun, M.~Haghpanahi, C.~Bourn, and A.~Y. Ng,
  ``Cardiologist-level arrhythmia detection with convolutional neural
  networks,'' \emph{arXiv preprint arXiv:1707.01836}, 2017.

\bibitem{Zhang2015a}
J.~Zhang and C.~Zong, ``Deep neural networks in machine translation: An
  overview,'' \emph{IEEE Intelligent Systems}, vol.~30, no.~5, pp. 16--25,
  2015.

\bibitem{He2016}
K.~He, X.~Zhang, S.~Ren, and J.~Sun, ``Deep residual learning for image
  recognition,'' in \emph{Proceedings of the IEEE conference on computer vision
  and pattern recognition}, 2016, pp. 770--778.

\bibitem{Zhang2015}
C.~Zhang, P.~Li, G.~Sun, Y.~Guan, B.~Xiao, and J.~Cong, ``Optimizing fpga-based
  accelerator design for deep convolutional neural networks,'' in
  \emph{Proceedings of the 2015 ACM/SIGDA International Symposium on
  Field-Programmable Gate Arrays}.\hskip 1em plus 0.5em minus 0.4em\relax ACM,
  2015, pp. 161--170.

\bibitem{Han2015}
S.~Han, H.~Mao, and W.~J. Dally, ``Deep compression: Compressing deep neural
  networks with pruning, trained quantization and huffman coding,'' in
  \emph{ICLR}, 2016.

\bibitem{Zhou2017a}
A.~Zhou, A.~Yao, Y.~Guo, L.~Xu, and Y.~Chen, ``Incremental network
  quantization: Towards lossless cnns with low-precision weights,''
  \emph{ICLR}, 2017.

\bibitem{Chen2017}
X.~Chen, X.~Hu, H.~Zhou, and N.~Xu, ``Fxpnet: Training a deep convolutional
  neural network in fixed-point representation,'' in \emph{Neural Networks
  (IJCNN), 2017 International Joint Conference on}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2017, pp. 2494--2501.

\bibitem{Lin2015a}
\BIBentryALTinterwordspacing
D.~D. Lin, S.~S. Talathi, and V.~S. Annapureddy, ``Fixed point quantization of
  deep convolutional networks,'' in \emph{Proceedings of the 33rd International
  Conference on International Conference on Machine Learning - Volume 48}, ser.
  ICML'16.\hskip 1em plus 0.5em minus 0.4em\relax JMLR.org, 2016, pp.
  2849--2858. [Online]. Available:
  \url{http://dl.acm.org/citation.cfm?id=3045390.3045690}
\BIBentrySTDinterwordspacing

\bibitem{Courbariaux2014}
M.~Courbariaux, Y.~Bengio, and J.-P. David, ``Training deep neural networks
  with low precision multiplications,'' \emph{arXiv preprint arXiv:1412.7024},
  2014.

\bibitem{Dong2017}
X.~Dong, S.~Chen, and S.~Pan, ``Learning to prune deep neural networks via
  layer-wise optimal brain surgeon,'' in \emph{Advances in Neural Information
  Processing Systems}, 2017, pp. 4860--4874.

\bibitem{Han2015a}
S.~Han, J.~Pool, J.~Tran, and W.~Dally, ``Learning both weights and connections
  for efficient neural networks,'' in \emph{Advances in neural information
  processing systems}, 2015, pp. 1135--1143.

\bibitem{Yang2017}
T.-J. Yang, Y.-H. Chen, and V.~Sze, ``Designing energy-efficient convolutional
  neural networks using energy-aware pruning,'' \emph{arXiv preprint}, 2017.

\bibitem{Yu2017}
J.~Yu, A.~Lukefahr, D.~Palframan, G.~Dasika, R.~Das, and S.~Mahlke, ``Scalpel:
  Customizing dnn pruning to the underlying hardware parallelism,'' in
  \emph{Proceedings of the 44th Annual International Symposium on Computer
  Architecture}.\hskip 1em plus 0.5em minus 0.4em\relax ACM, 2017, pp.
  548--560.

\bibitem{Gysel2016a}
P.~Gysel, ``Ristretto: Hardware-oriented approximation of convolutional neural
  networks,'' Master's thesis, University of California Davis, 2016.

\bibitem{Zhou2017}
Y.~Zhou, S.-M. Moosavi-Dezfooli, N.-M. Cheung, and P.~Frossard, ``Adaptive
  quantization for deep neural network,'' \emph{arXiv preprint
  arXiv:1712.01048}, 2017.

\bibitem{han2016eie}
S.~Han, X.~Liu, H.~Mao, J.~Pu, A.~Pedram, M.~A. Horowitz, and W.~J. Dally,
  ``Eie: efficient inference engine on compressed deep neural network,'' in
  \emph{Computer Architecture (ISCA), 2016 ACM/IEEE 43rd Annual International
  Symposium on}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2016, pp.
  243--254.

\bibitem{Gysel2016}
P.~Gysel, M.~Motamedi, and S.~Ghiasi, ``Hardware-oriented approximation of
  convolutional neural networks,'' \emph{arXiv preprint arXiv:1604.03168},
  2016.

\bibitem{Hubara2016}
I.~Hubara, M.~Courbariaux, D.~Soudry, R.~El-Yaniv, and Y.~Bengio, ``Quantized
  neural networks: Training neural networks with low precision weights and
  activations,'' \emph{ArXiv}, 2016.

\bibitem{Springenberg2015}
J.~T. Springenberg, A.~Dosovitskiy, T.~Brox, and M.~Riedmiller, ``Striving for
  simplicity: The all convolutional net,'' in \emph{ICLR}, 2015.

\bibitem{Shin2017}
S.~Shin, Y.~Boo, and W.~Sung, ``Fixed-point optimization of deep neural
  networks with adaptive step size retraining,'' \emph{ICASSP}, 2017.

\bibitem{Krizhevsky2009}
\BIBentryALTinterwordspacing
A.~Krizhevsky, V.~Nair, and G.~Hinton. (2009, mar) Cifar-10 and cifar-100
  datasets. [Online]. Available:
  \url{https://www.cs.toronto.edu/~kriz/cifar.html}
\BIBentrySTDinterwordspacing

\bibitem{netzer2011reading}
Y.~Netzer, T.~Wang, A.~Coates, A.~Bissacco, B.~Wu, and A.~Y. Ng, ``Reading
  digits in natural images with unsupervised feature learning,'' in \emph{NIPS
  workshop on deep learning and unsupervised feature learning}, vol. 2011,
  no.~2, 2011, p.~5.

\bibitem{Tang2017}
W.~Tang, G.~Hua, and L.~Wang, ``How to train a compact binary neural network
  with high accuracy?'' in \emph{AAAI}, 2017, pp. 2625--2631.

\bibitem{Courbariaux2015}
M.~Courbariaux, Y.~Bengio, and J.-P. David, ``Binaryconnect: Training deep
  neural networks with binary weights during propagations,'' in \emph{Advances
  in neural information processing systems}, 2015, pp. 3123--3131.

\bibitem{Gupta2015a}
S.~Gupta, A.~Agrawal, K.~Gopalakrishnan, and P.~Narayanan, ``Deep learning with
  limited numerical precision,'' in \emph{ICML}, 2015.

\end{thebibliography}
